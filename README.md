# ğŸ¯ Eloquence AI

**Multimodal AI Interface** - Real-time gesture detection, face tracking, emotion recognition, and speech analysis powered by MediaPipe, DeepFace, and LiveKit.

![Gesture Lab Demo](https://img.shields.io/badge/Status-Active-success)
![License](https://img.shields.io/badge/License-MIT-blue)
![Python](https://img.shields.io/badge/Python-3.12-blue)
![Next.js](https://img.shields.io/badge/Next.js-14-black)

---

## âœ¨ Features

### ğŸ–ï¸ **Gesture Lab**
- **7 Gesture Recognition**: FIST, OPEN_PALM, POINTING, PEACE, THUMBS_UP, OK, TCHAO
- **Real-time Landmarks Visualization**: Hand (21 points) + Face (468 points)
- **Multi-hand Detection**: Support for 2 hands simultaneously
- **Low Latency**: 50ms frame processing (20 FPS)
- **Visual Feedback**: Animated emoji overlays and pulse effects

### ğŸ˜Š **Emotion Recognition**
- Real-time facial emotion detection using DeepFace
- 7 emotions: Happy, Sad, Angry, Surprise, Fear, Disgust, Neutral
- Adaptive UI based on detected emotion
- Emotion history tracking

### ğŸ‘ï¸ **Gaze Tracking**
- Eye movement detection
- Attention monitoring
- Gaze deviation alerts

### ğŸ¤ **Speech Emotion Analysis** *(Coming Soon)*
- Voice emotion recognition
- Speech-to-text
- Intent detection

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.12+**
- **Node.js 18+**
- **npm or yarn**

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/jeandirel/eloquence-ai.git
cd eloquence-ai
```

2. **Run the startup script**
```bash
# Windows
.\start_gesture_lab.bat

# Or manually start services:
# Terminal 1 - MediaPipe Service
cd backend/services/mediapipe_service
python main.py

# Terminal 2 - Orchestrator
cd backend
python main.py

# Terminal 3 - Frontend
cd frontend
npm install
npm run dev
```

3. **Access the application**
- **Gesture Lab**: http://localhost:3000/gestures
- **Emotion Recognition**: http://localhost:3000/emotion
- **Main Dashboard**: http://localhost:3000

---

## ğŸ—ï¸ Architecture

```
eloquence-ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                    # Orchestrator (WebSocket hub)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ mediapipe_service/     # Gesture & gaze detection
â”‚   â”‚   â”œâ”€â”€ deepface_service/      # Emotion recognition
â”‚   â”‚   â””â”€â”€ audio_service/         # Speech processing
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ gestures/              # Gesture Lab interface
â”‚   â”‚   â”œâ”€â”€ emotion/               # Emotion detection UI
â”‚   â”‚   â””â”€â”€ page.tsx               # Main landing page
â”‚   â”œâ”€â”€ components/
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ start_gesture_lab.bat          # Quick start script
```

### Tech Stack

**Backend**
- FastAPI (WebSocket server)
- MediaPipe 0.10.14 (Hand & Face detection)
- DeepFace (Emotion recognition)
- NumPy, OpenCV

**Frontend**
- Next.js 14 (React framework)
- TypeScript
- Framer Motion (Animations)
- TailwindCSS
- Canvas API (Landmarks rendering)

---

## ğŸ® Usage

### Gesture Lab

1. Open http://localhost:3000/gestures
2. Allow camera access
3. Perform gestures in front of the camera:
   - âœŠ **FIST**: Close your hand
   - âœ‹ **OPEN_PALM**: Open all fingers
   - â˜ï¸ **POINTING**: Raise index finger only
   - âœŒï¸ **PEACE**: Victory sign (index + middle)
   - ğŸ‘ **THUMBS_UP**: Raise thumb only
   - ğŸ‘Œ **OK**: Circle with thumb + index
   - ğŸ‘‹ **TCHAO**: Open hand with lateral movement

### Emotion Recognition

1. Open http://localhost:3000/emotion
2. Allow camera access
3. Your facial emotion will be detected in real-time
4. UI adapts based on detected emotion

---

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| **Gesture Detection FPS** | 20 FPS |
| **Frame Latency** | 50ms |
| **Landmarks (Hand)** | 21 points |
| **Landmarks (Face)** | 468 points |
| **Supported Gestures** | 7 |
| **Validation Threshold** | 4/5 frames |

---

## ğŸ› ï¸ Configuration

### Environment Variables

Create a `.env` file in the `backend/` directory:

```env
# Orchestrator
ORCHESTRATOR_PORT=8000
ORCHESTRATOR_HOST=0.0.0.0

# MediaPipe Service
MEDIAPIPE_PORT=8002

# DeepFace Service
DEEPFACE_PORT=8003

# Audio Service
AUDIO_PORT=8001
```

---

## ğŸ“ API Reference

### WebSocket Endpoint

**URL**: `ws://localhost:8000/ws`

**Message Format** (Client â†’ Server):
```typescript
// Video frame
Blob([0x00, ...imageData])

// Audio chunk
Blob([0x01, ...audioData])
```

**Message Format** (Server â†’ Client):
```json
{
  "type": "UI_COMMAND",
  "source": "GESTURE",
  "command": "APPROVE",
  "gesture": "THUMBS_UP",
  "hand_landmarks": [[{"x": 0.5, "y": 0.5, "z": 0.0}, ...]],
  "face_landmarks": [[{"x": 0.5, "y": 0.3, "z": 0.0}, ...]],
  "hand_connections": [[0, 1], [1, 2], ...],
  "face_connections": [[0, 1], ...]
}
```

---

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest

# Frontend tests
cd frontend
npm test
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details

---

## ğŸ‘¤ Author

**Jean Direl**
- GitHub: [@jeandirel](https://github.com/jeandirel)
- Email: jedirkab70@gmail.com

---

## ğŸ™ Acknowledgments

- **MediaPipe** by Google for hand and face tracking
- **DeepFace** for emotion recognition
- **Next.js** team for the amazing framework
- **Framer Motion** for smooth animations

---

## ğŸ› Known Issues

- MediaPipe requires specific version (0.10.14) for `solutions` API
- Large model files excluded from git (see `.gitignore`)

---

## ğŸ”® Roadmap

- [ ] Custom gesture training
- [ ] Multi-user support
- [ ] 3D hand tracking
- [ ] Gesture sequence recognition
- [ ] Export recorded sessions
- [ ] Mobile support

---

## ğŸ“ Support

For issues and questions:
- Create an issue on [GitHub Issues](https://github.com/jeandirel/eloquence-ai/issues)
- Email: jedirkab70@gmail.com

---

**Made with â¤ï¸ for Human-Computer Interaction**
