# üìò OmniSense AI - Documentation Technique

**Version:** 2.0.0
**Date:** Janvier 2026
**Auteur:** OmniSense Team

---

## 1. Introduction

**OmniSense AI** est une plateforme d'interaction multimodale temps-r√©el con√ßue pour d√©montrer le futur des Interfaces Homme-Machine (HCI). Elle fusionne trois modalit√©s principales pour cr√©er une exp√©rience empathique et naturelle :
1.  **Vision (Gestes & Regard):** Contr√¥le sans contact.
2.  **Affective Computing (Emotions):** Adaptation de l'interface √† l'√©tat √©motionnel.
3.  **Vocal (Audio):** Analyse de l'intention et de la tonalit√© (en cours).

L'architecture repose sur des micro-services asynchrones orchestr√©s par un hub central.

---

## 2. Architecture Syst√®me

Le syst√®me suit une architecture **Micro-services Event-Driven**.

### Diagramme de Flux (Data Pipeline)

```mermaid
graph TD
    Client[üñ•Ô∏è Frontend (Next.js)] <-->|WebSocket Binary/JSON| Orch[üß† Orchestrator (FastAPI)]
    
    subgraph "Backend Services"
        Orch -->|HTTP POST (Frame)| MP[üñêÔ∏è MediaPipe Service]
        Orch -->|HTTP POST (Frame)| DF[üòä DeepFace Service]
        Orch -->|HTTP POST (Audio)| AU[üé§ Audio Service]
        
        MP -->|Landmarks + Gesture| Orch
        DF -->|Emotion + Confidence| Orch
        AU -->|Transcript + Intent| Orch
    end
    
    Orch -->|Fusion Logic| State[Session Manager]
    State -->|Unified Feedback| Orch
    Orch -->|UI Commands| Client
```

### Description des Composants

| Service | Port | R√¥le | Technologies Cl√©s |
|---------|------|------|-------------------|
| **Orchestrator** | `8000` | Hub WebSocket, Fusion des donn√©es, Gestion de Session. | FastAPI, Uvicorn, Asyncio |
| **MediaPipe** | `8002` | D√©tection squelettique (Mains + Visage). | MediaPipe Solutions, OpenCV |
| **DeepFace** | `8003` | Analyse √©motionnelle faciale. | DeepFace (VGG-Face), TensorFlow |
| **Audio** | `8001` | Transcription & Analyse tonale. | Wav2Vec2, Torch, Librosa |
| **Frontend** | `3000` | Interface Utilisateur & Capture. | Next.js, React, Tailwind, Canvas |

---

## 3. Stack Technique D√©taill√©e

### üß† Backend (Python 3.12+)

#### Orchestrator
*   `fastapi`: Framework API haute performance.
*   `websockets`: Gestion des connexions temps-r√©el.
*   `httpx`: Requ√™tes asynchrones vers les micro-services.

#### Vision Services (MediaPipe & DeepFace)
*   `mediapipe>=0.10.14`: Tracking biom√©trique.
*   `deepface`: Framework de reconnaissance faciale.
*   `opencv-python`: Traitement d'image (redimensionnement, conversion).
*   `numpy`: Calculs matriciels.

#### Audio Service
*   `transformers`: Mod√®les Hugging Face (Wav2Vec2).
*   `torch`: PyTorch pour l'inf√©rence audio.
*   `librosa`: Traitement du signal audio (chargement, resampling).
*   `scikit-learn`: Classification d'intentions simples (Logistic Regression).

### üñ•Ô∏è Frontend (Node.js 18+)
*   **Framework:** `Next.js 16` (App Router).
*   **Langage:** `TypeScript`.
*   **Styling:** `Tailwind CSS 4`.
*   **Animation:** `Framer Motion` (Transitions UI fluides).
*   **Data Viz:** `Recharts` (Graphiques Radar pour les √©motions).
*   **Ic√¥nes:** `Lucide React`.

---

## 4. Guide d'Installation

### Pr√©requis
*   Python 3.12 ou sup√©rieur.
*   Node.js 18 ou sup√©rieur.
*   Git.

### Installation Automatis√©e (Windows)

1.  **Cloner le d√©p√¥t :**
    ```bash
    git clone https://github.com/jeandirel/eloquence-ai.git
    cd eloquence-ai
    ```

2.  **Lancer le script de d√©marrage :**
    ```bash
    .\start_gesture_lab.bat
    ```
    *Ce script va automatiquement :*
    *   Cr√©er les environnements virtuels Python si n√©cessaire.
    *   Installer les d√©pendances (`pip install -r requirements.txt`).
    *   Installer les modules Node (`npm install`).
    *   Lancer les 3 terminaux (Backend Services, Orchestrator, Frontend).

---

## 5. Guide d'Utilisation

### üñêÔ∏è Gesture Lab (`/gestures`)
Module de reconnaissance gestuelle et de tracking.

*   **Fonctionnalit√©s :**
    *   Visualisation des landmarks (Mains + Visage).
    *   Feedback visuel des gestes d√©tect√©s.
*   **Gestes Support√©s :**
    *   ‚úä **FIST** (Poing ferm√©)
    *   ‚úã **OPEN_PALM** (Main ouverte)
    *   ‚òùÔ∏è **POINTING** (Index lev√©)
    *   ‚úåÔ∏è **PEACE** (V de la victoire)
    *   üëç **THUMBS_UP** (Pouce en l'air)
    *   üëå **OK** (Pouce et index joints)
    *   üëã **TCHAO** (Signe de la main)

### üòä Emotion AI (`/emotion`)
Module d'analyse affective.

*   **Radar Chart :** Visualisation en temps r√©el de la confiance pour 7 √©motions.
*   **Adaptive UI :**
    *   *Calm Mode* (Bleu fondu) si Tristesse d√©tect√©e.
    *   *Dynamic Mode* (Vibrant) si Joie/Surprise.
    *   *Simplified Mode* (Sombre) si Col√®re/Peur (r√©duction de la charge cognitive).

### üìä Session Reporting (Nouveau)
Disponible sur les pages Gesture et Emotion.

1.  Cliquez sur **"Start Session"** (Bouton Vert).
2.  Interagissez avec l'application.
3.  Cliquez sur **"Stop Session"** (Bouton Rouge).
4.  L'application g√©n√®re un **Rapport PDF-like** incluant :
    *   Dur√©e de la session.
    *   √âmotion dominante.
    *   Timeline des gestes effectu√©s.

---

## 6. R√©f√©rence API (WebSocket)

**Endpoint:** `ws://localhost:8000/ws`

### Protocole Client ‚Üí Serveur

Le client envoie des **Frames Binaires** pour la performance. Le premier octet d√©finit le type.

*   **Vid√©o :** `[0x00] + [JPEG Blob]`
*   **Audio :** `[0x01] + [PCM 16-bit Blob]`
*   **Contr√¥le :** `[0x02] + [JSON String]`

**Exemple Contr√¥le (Start Session):**
```json
{
  "type": "SESSION_CONTROL",
  "action": "START"
}
```

### Protocole Serveur ‚Üí Client

Le serveur r√©pond toujours en **JSON Texte**.

**Commande UI (Geste):**
```json
{
  "type": "UI_COMMAND",
  "source": "GESTURE",
  "command": "APPROVE",
  "gesture": "THUMBS_UP",
  "hand_landmarks": [...] // Array of {x, y, z}
}
```

**Adaptation UI (Emotion):**
```json
{
  "type": "UI_ADAPTATION",
  "emotion": "happy",
  "confidence": 0.98,
  "mode": "DYNAMIC"
}
```

**Rapport de Session:**
```json
{
  "type": "SESSION_REPORT",
  "report": {
    "duration_seconds": 45.2,
    "emotion_stats": {"happy": 120, "neutral": 50},
    "gesture_stats": {"OK": 2, "THUMBS_UP": 1}
  }
}
```

---

## 7. Troubleshooting

*   **Erreur "Camera access denied" :** V√©rifiez que votre navigateur a la permission d'acc√©der √† la webcam et qu'aucune autre application (Zoom, Teams) ne l'utilise.
*   **Latence √©lev√©e :** L'analyse faciale (DeepFace) est lourde. Si vous n'avez pas de GPU NVIDIA, la latence peut atteindre 200-300ms.
*   **Gestes non reconnus :** Assurez-vous d'avoir un bon √©clairage et que votre main est visible en entier dans le cadre.

---

**¬© 2026 OmniSense AI Project**
